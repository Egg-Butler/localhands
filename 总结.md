## 基于本地模型的openhands部署尝试。

### 模型的选择

在消费级显卡上，16gb显存已经是比较高的了，这个级别的显卡（比如作者自己用的5070ti）、以及24gb内存的M芯片mac大约能部署一个14B的量化的模型，并把上下文窗口开放到32K。
市面上的开源的更小尺寸的模型接上openhands框架后，均无法满足openhands的工具调用需求，在swe-bench lite上随机挑选20个后，一个都无法完成，很多都是在工具调用失败上浪费轮次，测试了的模型包括以下：
 - qwen2.5-coder-7b
 - llama3.1-7b
 - llama3.2-3b
 - deepseekR1-8b
 - gpt-oss-20b(工具调用格式暂不支持)

在openhands官网推荐了一款与其他机构合作的(devstral模型
)[https://huggingface.co/mistralai/Devstral-Small-2505]，号称在openhands框架下，在Swe-Bench Verified上有46.8%，但是参数量在24B，gguf在5070ti上推理也较慢，还需要swap到内存。另外也尝试用vllm部署了qwen3-14b-awq，但是推理速度过慢（3tokens/s）所以不得不放弃。

在以上的基础上，最终选择了qwen3-14b-gguf。使用lm studio部署，lm studio同时也是openhands推荐的平台。

### 测评的不完全

在原本的预期中，希望跑完整的swe_bench lite。但是在工作的一开始在模型的挑选时并没有想到时间的紧迫性，导致最后时间上完全无法完成整个测评。
另外在测评流程中，docker镜像拉取非常耗时，我个人在家中完全无法成功拉取测评集中代码仓所对应docker，导致最终只能使用我macbook在单位跑小尺寸模型拉取到的docker镜像的缓存进行非常小范围的测评。许多镜像由于取名完全是哈希值，且log被删除，可以通过log对应上的可能是对应缓存的镜像只有12个。且由于镜像在mac上，所以只能在mac上部署openhands，通过局域网连接到有5070ti显卡的主机上进行远程的本地推理。

### 结果的evaluation
评测结果原本应通过swe_bench官方的evaluation方法，即再次部署环境并跑用例进行验证，但是由于测评库没有arm_64版本，需要另外开发对接层，如在流程中接入rosetta2等，且无法拉取到对应docker镜像。故最终把成功跑出patch的结果提交给主流大尺寸llm进行评估。

### 对于效果的优化
整个openhands是针对于在线的大尺寸主流模型设计，但是还是在配置中预留了丰富的配置项。所以通过配置项，通过降低tools工具数量、增强上下午压缩次数、去掉跑swe_bench不需要的相关验证、并对原本的system prompt进行裁剪，去掉一些对于小尺寸模型不重要的功能后，在12个用例中效果大致提升了一倍。

### 最终结论
尽管最终可以跑出结果，但是本地推理结果并没有特别好，而且速度非常慢，模型在5070ti上一个任务需要跑15分钟以上，在macbook m2max版上需要30分钟以上。目前看来，用本地模型完成swe的复杂任务还不是非常靠谱，


