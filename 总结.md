## 基于本地模型的OpenHands部署尝试

### 摘要

本实验尝试在消费级硬件（RTX 5070Ti 16GB显存和MacBook M2 Max 24GB内存）上部署本地大语言模型，使用OpenHands框架完成SWE-bench Lite评测任务。经过多轮模型筛选和配置优化，最终选择Qwen3-14B-GGUF模型，在12个测试用例中实现了从22.2%到44.4%的patch生成率提升。然而，本地推理速度较慢（5070Ti上15分钟/任务，M2 Max上30分钟/任务），且受限于Docker镜像拉取和ARM64架构兼容性问题，未能完成完整的官方评估流程。实验表明，使用本地模型完成复杂的SWE任务在当前硬件条件下仍面临较大挑战。

### 1. 模型的选择

在消费级显卡上，16GB显存已经是比较高的配置了。这个级别的显卡（如作者使用的RTX 5070Ti）以及24GB内存的M芯片Mac，大约能部署一个14B参数量级的量化模型，并将上下文窗口开放到32K。

市面上的开源小尺寸模型接入OpenHands框架后，均无法满足工具调用需求。在SWE-bench Lite上随机挑选20个用例测试后，一个都无法完成，很多都在工具调用失败上浪费轮次。测试的模型包括：

- **qwen2.5-coder-7b** (7B参数): 工具调用失败，无法完成任务
- **llama3.1-7b** (7B参数): 工具调用失败，无法完成任务
- **llama3.2-3b** (3B参数): 工具调用失败，无法完成任务
- **deepseekR1-8b** (8B参数): 工具调用失败，无法完成任务
- **gpt-oss-20b** (20B参数): 工具调用格式暂不支持

OpenHands官网推荐了一款与其他机构合作的[devstral模型](https://huggingface.co/mistralai/Devstral-Small-2505)，号称在OpenHands框架下，在SWE-bench Verified上有46.8%的正确率。但该模型参数量为24B，GGUF格式在5070Ti上推理较慢，还需要swap到内存，实际使用体验不佳。

另外也尝试用vLLM部署了qwen3-14b-awq，但是推理速度过慢（约3 tokens/s），所以不得不放弃。

基于以上测试，最终选择了**qwen3-14b-gguf**模型，使用LM Studio部署。该模型在5070Ti上可以完全加载到显存中，不需要swap到内存。LM Studio同时也是OpenHands推荐的部署平台。

### 2. 遇到的挑战

#### 2.1 时间限制
在实验初期进行模型挑选时，没有充分考虑到时间紧迫性，导致后期无法完成完整的SWE-bench Lite评测。

#### 2.2 Docker镜像拉取问题
在测评流程中，Docker镜像拉取非常耗时。作者在家中完全无法成功拉取测评集中代码仓库对应的Docker镜像，导致最终只能使用MacBook在单位跑小尺寸模型时拉取到的Docker镜像缓存进行非常小范围的测评。

许多镜像由于命名完全是哈希值，且日志被删除，能够通过日志对应上的缓存镜像只有12个。由于镜像在Mac上，所以只能在Mac上部署OpenHands，通过局域网连接到有5070Ti显卡的主机上进行远程的本地推理。

#### 2.3 ARM64架构兼容性问题
评测结果原本应通过SWE-bench官方的evaluation方法，即再次部署环境并运行测试用例进行验证。但由于测评库没有ARM64版本，需要另外开发对接层（如在流程中接入Rosetta2等），且无法拉取到对应的Docker镜像。因此最终将成功生成的patch结果提交给主流大尺寸LLM进行评估。

#### 2.4 网络环境限制
在家中网络环境下，Docker镜像拉取失败率极高，严重影响了实验进度。

### 3. 测评的不完全

在原本的预期中，希望运行完整的SWE-bench Lite评测。但由于上述挑战，最终只完成了12个用例的测试，其中：
- Baseline配置：9个实例，2个成功生成patch（22.2%）
- Optimized配置：9个实例，4个成功生成patch（44.4%）

有3个期望的实例未能运行：
- django__django-11532
- astropy__astropy-11693
- pydata__xarray-6804

这些实例可能因为Docker镜像构建失败或其他原因未运行。

### 4. 结果的evaluation

评测结果原本应通过SWE-bench官方的evaluation方法进行验证，但由于测评库没有ARM64版本，需要另外开发对接层（如在流程中接入Rosetta2等），且无法拉取到对应的Docker镜像。

因此，最终采用了替代方案：将成功生成的patch结果提交给主流大尺寸LLM（如GPT-4、Claude等）进行评估，通过人工审核和模型评估来判断patch的质量和正确性。这种方法虽然不如官方评估方法严格，但在当前限制条件下是可行的替代方案。

**注意**：当前统计的是Patch生成率，不是正确率。Patch生成率表示是否生成了格式正确的git patch，而正确率需要验证patch能否成功应用并通过测试。

### 5. 对于效果的优化

整个OpenHands框架是针对在线的大尺寸主流模型设计的，但在配置中预留了丰富的配置项。通过以下优化策略，在12个用例中效果从22.2%提升到44.4%，提升了一倍：

#### 5.1 工具数量优化
- 降低了可用工具的数量，减少模型在工具选择上的困惑
- 移除了与SWE-bench任务无关的工具

#### 5.2 上下文压缩优化
- 增强了上下文压缩次数，减少长上下文对模型性能的影响
- 优化了上下文窗口的使用策略

#### 5.3 验证流程简化
- 去掉了运行SWE-bench不需要的相关验证步骤
- 简化了中间检查流程

#### 5.4 System Prompt裁剪
- 对原本的system prompt进行了裁剪
- 去掉了一些对于小尺寸模型不重要的功能描述
- 保留了核心的工具调用和代码生成指令

#### 5.5 性能对比
- Baseline配置：9个测试实例，2个成功生成patch，patch生成率为22.2%
- Optimized配置：9个测试实例，4个成功生成patch，patch生成率为44.4%
- 提升幅度：从22.2%提升到44.4%，提升了一倍（+22.2个百分点）

### 6. 性能分析

#### 6.1 推理速度
- **RTX 5070Ti (16GB显存)**: 平均每个任务需要15分钟以上
- **MacBook M2 Max (24GB内存)**: 平均每个任务需要30分钟以上
- **总耗时**: Baseline配置下，9个实例总耗时约39分40秒，平均每个实例约4.4分钟（包含Docker镜像构建时间）

#### 6.2 资源占用
- 模型大小：Qwen3-14B-GGUF量化版本
- 显存占用：在5070Ti上可以完全加载到16GB显存中，不需要swap到内存
- 内存占用：在MacBook M2 Max上使用24GB内存
- 上下文窗口：32K tokens

### 7. 最终结论

尽管最终可以跑出结果，但本地推理结果并没有特别好，而且速度非常慢。主要问题包括：

1. **性能限制**：模型在5070Ti上一个任务需要15分钟以上，在MacBook M2 Max上需要30分钟以上，远低于在线API的响应速度。

2. **成功率有限**：即使经过优化，patch生成率也只有44.4%，且这仅仅是生成率，不是正确率。实际通过测试的正确率可能更低。

3. **硬件要求高**：需要16GB以上显存或24GB以上内存，且推理速度仍然较慢。

4. **评估流程不完整**：受限于架构兼容性和Docker镜像拉取问题，无法完成官方的完整评估流程。

**结论**：目前看来，用本地模型完成SWE的复杂任务还不是非常靠谱，特别是在消费级硬件上。对于生产环境或大规模评测，仍建议使用在线的大尺寸模型API。但对于研究、学习或特定场景下的本地部署需求，通过合理的配置优化，本地模型仍有一定的实用价值。

### 8. 经验教训

1. **模型选择的重要性**：小尺寸模型（7B以下）在复杂工具调用任务上表现不佳，至少需要14B以上的模型才能基本满足需求。

2. **配置优化的价值**：通过合理的配置优化（减少工具、压缩上下文、裁剪prompt），可以将性能提升一倍，说明框架的可配置性很重要。

3. **评估流程的完整性**：完整的评估流程需要考虑架构兼容性、Docker镜像可用性等多个因素，这些在实验初期就应该充分考虑。

4. **时间规划的重要性**：模型筛选和配置优化需要大量时间，应该提前规划好时间分配。

5. **硬件限制的现实**：消费级硬件虽然可以运行，但速度和效果都有明显限制，需要合理设置期望值。

### 9. 下一步

1. **继续完成全量测评**：解决Docker镜像拉取和ARM64架构兼容性问题，完成完整的SWE-bench Lite评测，获得更全面的性能评估数据。

2. **优化工具以适应更小尺寸模型**：进一步优化OpenHands的工具调用机制和prompt设计，使其能够更好地适配7B-8B等更小尺寸的模型，降低硬件门槛。

3. **局域网多个本地模型联合构建agent流程**：探索在局域网环境下，将多个本地模型（如5070Ti上的模型和MacBook上的模型）联合使用，通过模型协作或任务分配的方式构建更强大的agent系统。