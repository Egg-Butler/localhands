#!/bin/bash

# è®¾ç½®ä»£ç†
export https_proxy=http://127.0.0.1:7890
export http_proxy=http://127.0.0.1:7890
export all_proxy=socks5://127.0.0.1:7890

echo "========================================"
echo "OpenHands 10åˆ†é’Ÿå¿«é€Ÿæµ‹è¯• (Llama-3.1-8Bæ¨¡å‹)"
echo "========================================"
echo ""
echo "ğŸ“‹ é…ç½®:"
echo "   æ¨¡å‹: llama3.1:8b-instruct-q4_0"
echo "   å®ä¾‹æ•°: 5 ä¸ª"
echo "   æœ€å¤§è¿­ä»£: 25 æ¬¡"
echo "   é¢„è®¡æ—¶é—´: 8-12 åˆ†é’Ÿ"
echo ""
echo "ğŸ’¡ Llama-3.1-8Bæ¨¡å‹ç‰¹ç‚¹:"
echo "   âœ… æ›´å¥½çš„æŒ‡ä»¤ç†è§£èƒ½åŠ›"
echo "   âœ… ç»è¿‡æŒ‡ä»¤å¾®è°ƒ"
echo "   âš ï¸  ä¸æ”¯æŒåŸç”Ÿå·¥å…·è°ƒç”¨ (ä½¿ç”¨mock function calling)"
echo "   âš ï¸  å¯èƒ½æ¯”qwen2.5-coderåœ¨å·¥å…·è°ƒç”¨æ ¼å¼ä¸Šè¡¨ç°æ›´å¥½"
echo ""

# å·¥ä½œç›®å½•
WORK_DIR="/Users/bitfun/codes/closehands/OpenHands"
cd "$WORK_DIR" || exit 1

# è¯„æµ‹é…ç½® - ä½¿ç”¨Llama-3.1æ¨¡å‹
MODEL_CONFIG="eval_local_model_llama3"
AGENT="CodeActAgent"
EVAL_LIMIT=5
MAX_ITER=25
NUM_WORKERS=1
DATASET="princeton-nlp/SWE-bench_Lite"
DATASET_SPLIT="test"

# æ£€æŸ¥ Ollama æœåŠ¡
echo "ğŸ” æ£€æŸ¥ Ollama æœåŠ¡..."
if ! curl -s http://localhost:11434/api/tags &> /dev/null; then
    echo "âŒ Ollama æœåŠ¡æœªè¿è¡Œ"
    echo "   è¯·è¿è¡Œ: ollama serve"
    exit 1
fi
echo "âœ… Ollama æœåŠ¡æ­£å¸¸"
echo ""

# æ£€æŸ¥Llama-3.1æ¨¡å‹
echo "ğŸ” æ£€æŸ¥ Llama-3.1 æ¨¡å‹..."
if ! ollama list | grep -q "llama3.1:8b-instruct-q4_0"; then
    echo "âŒ Llama-3.1 æ¨¡å‹æœªå®‰è£…"
    echo "   æ­£åœ¨ä¸‹è½½ llama3.1:8b-instruct-q4_0 (çº¦ 4.7GB)..."
    ollama pull llama3.1:8b-instruct-q4_0
    if [ $? -ne 0 ]; then
        echo "âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥"
        echo "   å°è¯•å…¶ä»–åç§°: llama3.1:8b"
        ollama pull llama3.1:8b
        if [ $? -ne 0 ]; then
            echo "âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            exit 1
        fi
        # æ›´æ–°é…ç½®ä½¿ç”¨å¤‡ç”¨æ¨¡å‹åç§°
        sed -i '' 's/llama3.1:8b-instruct-q4_0/llama3.1:8b/g' config.toml
    fi
fi
echo "âœ… Llama-3.1 æ¨¡å‹å‡†å¤‡å°±ç»ª"
echo ""

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR="$WORK_DIR/evaluation/evaluation_outputs/quick_llama3_10min_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$OUTPUT_DIR"

# è®¾ç½®ç¯å¢ƒå˜é‡
export WORKSPACE_BASE="$WORK_DIR/workspace"
export CACHE_DIR="$WORK_DIR/cache"
export EVAL_OUTPUT_DIR="$OUTPUT_DIR"
export EVAL_NOTE="quick-llama3-10min-test"

# è®°å½•å¼€å§‹æ—¶é—´
START_TIME=$(date +%s)
echo "â±ï¸  å¼€å§‹æ—¶é—´: $(date '+%H:%M:%S')"
echo ""
echo "ğŸš€ å¼€å§‹å¿«é€Ÿæµ‹è¯• (ä½¿ç”¨Llama-3.1æ¨¡å‹)..."
echo ""

# è¿è¡Œè¯„æµ‹
poetry run python evaluation/benchmarks/swe_bench/run_infer.py \
    --llm-config "$MODEL_CONFIG" \
    --agent-cls "$AGENT" \
    --max-iterations "$MAX_ITER" \
    --eval-num-workers "$NUM_WORKERS" \
    --eval-n-limit "$EVAL_LIMIT" \
    --data-split "$DATASET_SPLIT" \
    --dataset-name "$DATASET"

# æ£€æŸ¥ç»“æœ
if [ $? -eq 0 ]; then
    END_TIME=$(date +%s)
    DURATION=$((END_TIME - START_TIME))
    MINUTES=$((DURATION / 60))
    SECONDS=$((DURATION % 60))
    
    echo ""
    echo "========================================"
    echo "âœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ!"
    echo "========================================"
    echo "â±ï¸  å®é™…ç”¨æ—¶: ${MINUTES}åˆ†${SECONDS}ç§’"
    echo "ğŸ“ ç»“æœç›®å½•: $OUTPUT_DIR"
    echo ""
    
    # æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶
    OUTPUT_JSONL=$(find "$WORK_DIR/evaluation/evaluation_outputs" -name "output.jsonl" -path "*llama3*" | head -1)
    
    if [ -n "$OUTPUT_JSONL" ]; then
        echo "ğŸ“Š è¾“å‡ºæ–‡ä»¶: $OUTPUT_JSONL"
        echo ""
        echo "ğŸ“ˆ å¿«é€Ÿç»Ÿè®¡:"
        TOTAL=$(wc -l < "$OUTPUT_JSONL" 2>/dev/null || echo "0")
        echo "   å®Œæˆå®ä¾‹: $TOTAL ä¸ª"
        echo ""
        echo "ğŸ’¡ ä¸‹ä¸€æ­¥:"
        echo "   1. æŸ¥çœ‹ç»“æœ: head -1 $OUTPUT_JSONL | python3 -m json.tool | head -50"
        echo "   2. å¯¹æ¯”7Bå’Œ14Bæ¨¡å‹ç»“æœ"
    fi
else
    echo ""
    echo "âŒ æµ‹è¯•å¤±è´¥,è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯"
    exit 1
fi

echo ""

